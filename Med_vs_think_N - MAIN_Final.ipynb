{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186fe094",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.preprocessing import ICA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.signal import welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b867a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Define a function to extract labels from filenames\n",
    "def extract_label_from_filename(filename):\n",
    "    # Assuming filenames are in the format: subject_task.bdf\n",
    "    task = filename.split('_')[1].split('.')[0].split('-')[1]\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c2210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Convert to DataFrame\n",
    "def convert_bdf_to_dataframe(bdf_filename):\n",
    "    \n",
    "    # Loading Data\n",
    "    raw_data = mne.io.read_raw_bdf(bdf_filename, preload=True)\n",
    "    ## raw_data._data = raw_data._data ** 2\n",
    "    \n",
    "    # ICA\n",
    "    ## n_components = 15\n",
    "    ## ica = ICA(n_components=n_components, random_state=97, max_iter=800)\n",
    "    ## ica.fit(raw_data)\n",
    "    \n",
    "    # Exclude components\n",
    "    ## components_to_exclude = [7, 9]\n",
    "    ## raw_cleaned = ica.apply(raw_data.copy(), exclude=components_to_exclude)\n",
    "    \n",
    "    # convert to dataframe\n",
    "    eeg_data_raw = raw_cleaned.get_data()\n",
    "    channel_names = raw_cleaned.ch_names\n",
    "    time_index = raw_cleaned.times\n",
    "        \n",
    "    eeg_data = pd.DataFrame(data=eeg_data_raw.T, columns=channel_names, index=time_index)\n",
    "    col_names = ['O1', 'O2', 'F3', 'F4', 'C3' ,'C4' , 'Fp1', 'Fp2']\n",
    "    eeg_data = eeg_data[col_names]\n",
    "    \n",
    "    # Group by each second\n",
    "    segment_size = 1024\n",
    "    num_segments = len(eeg_data) // segment_size\n",
    "    reduced_df = pd.DataFrame(columns=eeg_data.columns)\n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = (i + 1) * segment_size\n",
    "        segment_data = eeg_data.iloc[start_idx:end_idx]    \n",
    "        mean_values = segment_data.mean()\n",
    "        reduced_df = reduced_df.append(mean_values, ignore_index=True)\n",
    "\n",
    "    return reduced_df, raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a0038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_cols = ['F3', 'F4', 'C3' ,'C4' , 'Fp1', 'Fp2', 'O1', 'O2']\n",
    "main_df_cols = []\n",
    "\n",
    "for col in req_cols:\n",
    "    if col == 'O1' or col == 'O2':\n",
    "        main_df_cols.append(f'{col} gamma mean')\n",
    "        main_df_cols.append(f'{col} gamma psd_mean')\n",
    "    else:\n",
    "        main_df_cols.append(f'{col} alpha mean')\n",
    "        main_df_cols.append(f'{col} alpha psd_mean')\n",
    "        main_df_cols.append(f'{col} beta mean')\n",
    "        main_df_cols.append(f'{col} beta psd_mean')\n",
    "main_df_cols.append('task')\n",
    "        \n",
    "main_df = pd.DataFrame(columns=main_df_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401465d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F3 alpha mean</th>\n",
       "      <th>F3 alpha psd_mean</th>\n",
       "      <th>F3 beta mean</th>\n",
       "      <th>F3 beta psd_mean</th>\n",
       "      <th>F4 alpha mean</th>\n",
       "      <th>F4 alpha psd_mean</th>\n",
       "      <th>F4 beta mean</th>\n",
       "      <th>F4 beta psd_mean</th>\n",
       "      <th>C3 alpha mean</th>\n",
       "      <th>C3 alpha psd_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Fp1 beta psd_mean</th>\n",
       "      <th>Fp2 alpha mean</th>\n",
       "      <th>Fp2 alpha psd_mean</th>\n",
       "      <th>Fp2 beta mean</th>\n",
       "      <th>Fp2 beta psd_mean</th>\n",
       "      <th>O1 gamma mean</th>\n",
       "      <th>O1 gamma psd_mean</th>\n",
       "      <th>O2 gamma mean</th>\n",
       "      <th>O2 gamma psd_mean</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [F3 alpha mean, F3 alpha psd_mean, F3 beta mean, F3 beta psd_mean, F4 alpha mean, F4 alpha psd_mean, F4 beta mean, F4 beta psd_mean, C3 alpha mean, C3 alpha psd_mean, C3 beta mean, C3 beta psd_mean, C4 alpha mean, C4 alpha psd_mean, C4 beta mean, C4 beta psd_mean, Fp1 alpha mean, Fp1 alpha psd_mean, Fp1 beta mean, Fp1 beta psd_mean, Fp2 alpha mean, Fp2 alpha psd_mean, Fp2 beta mean, Fp2 beta psd_mean, O1 gamma mean, O1 gamma psd_mean, O2 gamma mean, O2 gamma psd_mean, task]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57119828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_and_psd_mean(reduced_decomp_df, label):\n",
    "    values = []\n",
    "    req_channels = reduced_decomp_df.columns\n",
    "    for channel in req_channels:\n",
    "        mean_val = reduced_decomp_df[channel].mean()\n",
    "        _, psd = welch(reduced_decomp_df[channel], fs=256)\n",
    "        values.append(mean_val)\n",
    "        values.append(psd.mean())\n",
    "    values.append(label)\n",
    "    main_df.loc[main_df.shape[0]] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c585306",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir1 = '../Rishikest_MIT_Dataset/'# Replace with the path to your .bdf data directory (/Dataset - 3/)\n",
    "data_dir2 = './Dataset-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbfa26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Initialize empty lists to store data and labels\n",
    "data = []\n",
    "labels = []\n",
    "raw_eegdata = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(data_dir1):\n",
    "    if filename.endswith('.bdf'):\n",
    "        # Convert the .bdf file to a DataFrame\n",
    "        eeg_data, eeg_raw = convert_bdf_to_dataframe(os.path.join(data_dir1, filename))\n",
    "        \n",
    "        # Extract labels from filenames\n",
    "        label = extract_label_from_filename(filename)\n",
    "#         # Append data and labels\n",
    "        raw_eegdata.append(eeg_raw)\n",
    "    \n",
    "        channel_name_1 = ['O1', 'O2']\n",
    "        channel_name_2 = ['F3', 'F4','C3' ,'C4' , 'Fp1', 'Fp2']  # Add more channel names as needed\n",
    "\n",
    "#         # Create dictionaries to store the filtered data\n",
    "        eeg_dataframe = pd.DataFrame()\n",
    "\n",
    "\n",
    "        # Loop through each channel and filter data\n",
    "        for channel_name in channel_name_2:\n",
    "            eeg_channel = eeg_raw.copy().pick_channels([channel_name])\n",
    "    \n",
    "            # Filter for Alpha (8-13 Hz)\n",
    "            alpha_filtered = eeg_channel.filter(l_freq=8, h_freq=13)\n",
    "            alpha_decomp = alpha_filtered.get_data()[0]\n",
    "            eeg_dataframe[f'{channel_name} alpha'] = alpha_decomp\n",
    "    \n",
    "            # Filter for Beta (13-30 Hz)\n",
    "            beta_filtered = eeg_channel.filter(l_freq=13, h_freq=30)\n",
    "            beta_decomp = beta_filtered.get_data()[0]\n",
    "            eeg_dataframe[f'{channel_name} beta'] = beta_decomp\n",
    "    \n",
    "        for channel_name in channel_name_1:\n",
    "            eeg_channel = eeg_raw.copy().pick_channels([channel_name])\n",
    "    \n",
    "            # Filter for Gamma (30-40 Hz)\n",
    "            gamma_filtered = eeg_channel.filter(l_freq=30, h_freq=40)\n",
    "            gamma_decomp = gamma_filtered.get_data()[0]\n",
    "            eeg_dataframe[f'{channel_name} gamma'] = gamma_decomp\n",
    "\n",
    "        # Grouping data for each second\n",
    "        segment_size = 1024\n",
    "        num_segments = len(eeg_dataframe) // segment_size\n",
    "        reduced_decomp_df = pd.DataFrame(columns=eeg_dataframe.columns)\n",
    "        for i in range(num_segments):\n",
    "            start_idx = i * segment_size\n",
    "            end_idx = (i + 1) * segment_size\n",
    "            segment_data = eeg_dataframe.iloc[start_idx:end_idx]    \n",
    "            mean_values = segment_data.mean()\n",
    "            sampling_frequency = 1024  \n",
    "            reduced_decomp_df = reduced_decomp_df.append(mean_values, ignore_index=True)\n",
    "        \n",
    "        extract_mean_and_psd_mean(reduced_decomp_df, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da677140",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_csv('main_df3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Initialize empty lists to store data and labels\n",
    "data = []\n",
    "labels = []\n",
    "raw_eegdata = []\n",
    "\n",
    "for filename in os.listdir(data_dir2):\n",
    "    if filename.endswith('.bdf'):\n",
    "        # Convert the .bdf file to a DataFrame\n",
    "        eeg_data, eeg_raw = convert_bdf_to_dataframe(os.path.join(data_dir2, filename))\n",
    "        \n",
    "        # Extract labels from filenames\n",
    "        label = extract_label_from_filename(filename)\n",
    "#         # Append data and labels\n",
    "        raw_eegdata.append(eeg_raw)\n",
    "    \n",
    "        channel_name_1 = ['O1', 'O2']\n",
    "        channel_name_2 = ['F3', 'F4','C3' ,'C4' , 'Fp1', 'Fp2']  # Add more channel names as needed\n",
    "\n",
    "#         # Create dictionaries to store the filtered data\n",
    "        eeg_dataframe = pd.DataFrame()\n",
    "\n",
    "\n",
    "        # Loop through each channel and filter data\n",
    "        for channel_name in channel_name_2:\n",
    "            eeg_channel = eeg_raw.copy().pick_channels([channel_name])\n",
    "    \n",
    "            # Filter for Alpha (8-13 Hz)\n",
    "            alpha_filtered = eeg_channel.filter(l_freq=8, h_freq=13)\n",
    "            alpha_decomp = alpha_filtered.get_data()[0]\n",
    "            eeg_dataframe[f'{channel_name} alpha'] = alpha_decomp\n",
    "    \n",
    "            # Filter for Beta (13-30 Hz)\n",
    "            beta_filtered = eeg_channel.filter(l_freq=13, h_freq=30)\n",
    "            beta_decomp = beta_filtered.get_data()[0]\n",
    "            eeg_dataframe[f'{channel_name} beta'] = beta_decomp\n",
    "    \n",
    "        for channel_name in channel_name_1:\n",
    "            eeg_channel = eeg_raw.copy().pick_channels([channel_name])\n",
    "    \n",
    "            # Filter for Gamma (30-40 Hz)\n",
    "            gamma_filtered = eeg_channel.filter(l_freq=30, h_freq=40)\n",
    "            gamma_decomp = gamma_filtered.get_data()[0]\n",
    "            eeg_dataframe[f'{channel_name} gamma'] = gamma_decomp\n",
    "\n",
    "        # Grouping data for each second\n",
    "        segment_size = 1024\n",
    "        num_segments = len(eeg_dataframe) // segment_size\n",
    "        reduced_decomp_df = pd.DataFrame(columns=eeg_dataframe.columns)\n",
    "        for i in range(num_segments):\n",
    "            start_idx = i * segment_size\n",
    "            end_idx = (i + 1) * segment_size\n",
    "            segment_data = eeg_dataframe.iloc[start_idx:end_idx]    \n",
    "            mean_values = segment_data.mean()\n",
    "            sampling_frequency = 1024  \n",
    "            reduced_decomp_df = reduced_decomp_df.append(mean_values, ignore_index=True)\n",
    "        \n",
    "        extract_mean_and_psd_mean(reduced_decomp_df, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_csv('main_df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc781eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('main_df3.csv')\n",
    "\n",
    "# Load the second CSV file into another DataFrame\n",
    "df2 = pd.read_csv('merged_dataset.csv')\n",
    "\n",
    "# Append df2 to df1\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('merged_dataset_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb779e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "421b3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.drop('Unnamed: 0', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cf6c698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F3 alpha mean</th>\n",
       "      <th>F3 alpha psd_mean</th>\n",
       "      <th>F3 beta mean</th>\n",
       "      <th>F3 beta psd_mean</th>\n",
       "      <th>F4 alpha mean</th>\n",
       "      <th>F4 alpha psd_mean</th>\n",
       "      <th>F4 beta mean</th>\n",
       "      <th>F4 beta psd_mean</th>\n",
       "      <th>C3 alpha mean</th>\n",
       "      <th>C3 alpha psd_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Fp1 beta psd_mean</th>\n",
       "      <th>Fp2 alpha mean</th>\n",
       "      <th>Fp2 alpha psd_mean</th>\n",
       "      <th>Fp2 beta mean</th>\n",
       "      <th>Fp2 beta psd_mean</th>\n",
       "      <th>O1 gamma mean</th>\n",
       "      <th>O1 gamma psd_mean</th>\n",
       "      <th>O2 gamma mean</th>\n",
       "      <th>O2 gamma psd_mean</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.513743e-13</td>\n",
       "      <td>3.726133e-22</td>\n",
       "      <td>-2.682559e-14</td>\n",
       "      <td>3.288808e-23</td>\n",
       "      <td>-9.515559e-14</td>\n",
       "      <td>5.562550e-21</td>\n",
       "      <td>-5.775861e-13</td>\n",
       "      <td>7.106336e-22</td>\n",
       "      <td>-6.227103e-13</td>\n",
       "      <td>6.441535e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301683e-21</td>\n",
       "      <td>2.104581e-12</td>\n",
       "      <td>3.491705e-19</td>\n",
       "      <td>-8.972747e-12</td>\n",
       "      <td>3.815544e-20</td>\n",
       "      <td>1.462627e-12</td>\n",
       "      <td>8.390667e-23</td>\n",
       "      <td>-1.834673e-13</td>\n",
       "      <td>1.259452e-24</td>\n",
       "      <td>med1breath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.641169e-12</td>\n",
       "      <td>4.587082e-21</td>\n",
       "      <td>-9.250070e-13</td>\n",
       "      <td>5.641363e-22</td>\n",
       "      <td>7.079107e-12</td>\n",
       "      <td>3.216913e-20</td>\n",
       "      <td>-5.932923e-14</td>\n",
       "      <td>3.037872e-21</td>\n",
       "      <td>1.046693e-12</td>\n",
       "      <td>7.630595e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.351102e-21</td>\n",
       "      <td>1.886877e-11</td>\n",
       "      <td>4.980549e-19</td>\n",
       "      <td>-1.780069e-12</td>\n",
       "      <td>4.427011e-20</td>\n",
       "      <td>-4.275238e-13</td>\n",
       "      <td>1.377218e-22</td>\n",
       "      <td>-2.358534e-13</td>\n",
       "      <td>1.524009e-23</td>\n",
       "      <td>med2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.110026e-12</td>\n",
       "      <td>9.769720e-21</td>\n",
       "      <td>9.964152e-14</td>\n",
       "      <td>4.792419e-22</td>\n",
       "      <td>-4.087955e-12</td>\n",
       "      <td>8.125469e-20</td>\n",
       "      <td>5.467051e-13</td>\n",
       "      <td>4.184450e-21</td>\n",
       "      <td>-1.788294e-12</td>\n",
       "      <td>1.903302e-20</td>\n",
       "      <td>...</td>\n",
       "      <td>1.484600e-21</td>\n",
       "      <td>-9.369546e-12</td>\n",
       "      <td>2.936030e-19</td>\n",
       "      <td>1.199184e-12</td>\n",
       "      <td>1.653221e-20</td>\n",
       "      <td>1.045572e-14</td>\n",
       "      <td>6.581794e-27</td>\n",
       "      <td>-9.197855e-14</td>\n",
       "      <td>7.589472e-24</td>\n",
       "      <td>think1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.985656e-12</td>\n",
       "      <td>2.873449e-21</td>\n",
       "      <td>2.032677e-13</td>\n",
       "      <td>2.973924e-22</td>\n",
       "      <td>-6.230472e-12</td>\n",
       "      <td>3.910542e-20</td>\n",
       "      <td>1.097228e-12</td>\n",
       "      <td>3.867061e-21</td>\n",
       "      <td>-2.167441e-12</td>\n",
       "      <td>8.275744e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.339161e-21</td>\n",
       "      <td>-1.426302e-11</td>\n",
       "      <td>1.362751e-19</td>\n",
       "      <td>1.125618e-12</td>\n",
       "      <td>1.455048e-20</td>\n",
       "      <td>9.151191e-16</td>\n",
       "      <td>4.195413e-27</td>\n",
       "      <td>-1.023871e-13</td>\n",
       "      <td>3.604797e-24</td>\n",
       "      <td>think2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.528760e-12</td>\n",
       "      <td>3.527670e-20</td>\n",
       "      <td>-1.353519e-12</td>\n",
       "      <td>1.106202e-21</td>\n",
       "      <td>9.481486e-12</td>\n",
       "      <td>4.706429e-20</td>\n",
       "      <td>-1.541731e-12</td>\n",
       "      <td>1.674903e-21</td>\n",
       "      <td>4.119992e-13</td>\n",
       "      <td>1.018550e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.603727e-21</td>\n",
       "      <td>1.986007e-11</td>\n",
       "      <td>2.720955e-19</td>\n",
       "      <td>-3.152343e-12</td>\n",
       "      <td>8.206781e-21</td>\n",
       "      <td>-7.601483e-14</td>\n",
       "      <td>9.007884e-25</td>\n",
       "      <td>1.798727e-14</td>\n",
       "      <td>1.490968e-24</td>\n",
       "      <td>med1breath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.379280e-12</td>\n",
       "      <td>4.423076e-20</td>\n",
       "      <td>1.223617e-12</td>\n",
       "      <td>1.180685e-21</td>\n",
       "      <td>-3.906943e-12</td>\n",
       "      <td>8.441385e-20</td>\n",
       "      <td>1.021773e-12</td>\n",
       "      <td>2.495726e-21</td>\n",
       "      <td>2.212569e-13</td>\n",
       "      <td>7.981192e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.377544e-21</td>\n",
       "      <td>-7.881025e-12</td>\n",
       "      <td>3.264408e-19</td>\n",
       "      <td>2.327577e-12</td>\n",
       "      <td>8.140498e-21</td>\n",
       "      <td>-3.391651e-14</td>\n",
       "      <td>1.063922e-24</td>\n",
       "      <td>-8.547079e-14</td>\n",
       "      <td>2.196102e-23</td>\n",
       "      <td>med2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-4.587835e-12</td>\n",
       "      <td>4.330082e-20</td>\n",
       "      <td>1.727180e-12</td>\n",
       "      <td>3.195574e-21</td>\n",
       "      <td>-3.870291e-12</td>\n",
       "      <td>4.189717e-20</td>\n",
       "      <td>1.403395e-12</td>\n",
       "      <td>3.225566e-21</td>\n",
       "      <td>-5.293694e-13</td>\n",
       "      <td>1.091890e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.432964e-21</td>\n",
       "      <td>-1.996213e-11</td>\n",
       "      <td>7.977403e-19</td>\n",
       "      <td>1.147115e-11</td>\n",
       "      <td>4.953088e-20</td>\n",
       "      <td>1.190654e-13</td>\n",
       "      <td>3.554555e-23</td>\n",
       "      <td>2.474980e-13</td>\n",
       "      <td>2.225727e-23</td>\n",
       "      <td>think1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.329518e-12</td>\n",
       "      <td>3.092606e-20</td>\n",
       "      <td>-5.441050e-13</td>\n",
       "      <td>3.797635e-21</td>\n",
       "      <td>1.053348e-12</td>\n",
       "      <td>2.256651e-20</td>\n",
       "      <td>-1.160170e-13</td>\n",
       "      <td>2.483586e-21</td>\n",
       "      <td>3.385722e-13</td>\n",
       "      <td>7.190826e-23</td>\n",
       "      <td>...</td>\n",
       "      <td>1.303909e-21</td>\n",
       "      <td>1.977747e-12</td>\n",
       "      <td>4.945291e-19</td>\n",
       "      <td>-1.645501e-12</td>\n",
       "      <td>4.143383e-20</td>\n",
       "      <td>-2.415048e-13</td>\n",
       "      <td>5.554593e-24</td>\n",
       "      <td>-4.687246e-14</td>\n",
       "      <td>1.324148e-24</td>\n",
       "      <td>think2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.464838e-12</td>\n",
       "      <td>3.998122e-20</td>\n",
       "      <td>7.614474e-14</td>\n",
       "      <td>9.044639e-21</td>\n",
       "      <td>-2.269418e-12</td>\n",
       "      <td>3.074925e-20</td>\n",
       "      <td>-4.057946e-13</td>\n",
       "      <td>6.314228e-21</td>\n",
       "      <td>-2.429279e-12</td>\n",
       "      <td>3.773124e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.277685e-21</td>\n",
       "      <td>1.529181e-12</td>\n",
       "      <td>1.355524e-20</td>\n",
       "      <td>9.406878e-13</td>\n",
       "      <td>2.428333e-21</td>\n",
       "      <td>-6.834998e-14</td>\n",
       "      <td>3.473931e-24</td>\n",
       "      <td>1.198150e-13</td>\n",
       "      <td>6.617865e-23</td>\n",
       "      <td>med1breath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-6.682706e-13</td>\n",
       "      <td>4.353180e-20</td>\n",
       "      <td>1.798419e-12</td>\n",
       "      <td>6.372825e-21</td>\n",
       "      <td>-2.334771e-12</td>\n",
       "      <td>2.913618e-20</td>\n",
       "      <td>-3.185245e-12</td>\n",
       "      <td>3.965142e-21</td>\n",
       "      <td>-8.880180e-13</td>\n",
       "      <td>1.680661e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.423573e-21</td>\n",
       "      <td>5.059446e-13</td>\n",
       "      <td>1.122695e-20</td>\n",
       "      <td>-1.420028e-12</td>\n",
       "      <td>1.473201e-21</td>\n",
       "      <td>-1.842385e-14</td>\n",
       "      <td>1.510362e-24</td>\n",
       "      <td>4.958239e-13</td>\n",
       "      <td>2.337877e-23</td>\n",
       "      <td>med2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.675986e-11</td>\n",
       "      <td>4.661575e-20</td>\n",
       "      <td>-4.681929e-12</td>\n",
       "      <td>7.481881e-21</td>\n",
       "      <td>3.538875e-12</td>\n",
       "      <td>2.861831e-20</td>\n",
       "      <td>1.164432e-12</td>\n",
       "      <td>4.348650e-21</td>\n",
       "      <td>6.727053e-12</td>\n",
       "      <td>1.658058e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.378970e-21</td>\n",
       "      <td>-1.121567e-12</td>\n",
       "      <td>1.047478e-20</td>\n",
       "      <td>-1.432469e-13</td>\n",
       "      <td>1.557606e-21</td>\n",
       "      <td>1.671160e-13</td>\n",
       "      <td>9.341824e-24</td>\n",
       "      <td>-5.916038e-14</td>\n",
       "      <td>2.708215e-23</td>\n",
       "      <td>think1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.920095e-12</td>\n",
       "      <td>4.483697e-20</td>\n",
       "      <td>4.948356e-13</td>\n",
       "      <td>6.069827e-21</td>\n",
       "      <td>-2.728418e-12</td>\n",
       "      <td>2.033302e-20</td>\n",
       "      <td>-4.279024e-13</td>\n",
       "      <td>2.608493e-21</td>\n",
       "      <td>-8.722756e-13</td>\n",
       "      <td>7.994848e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.489864e-21</td>\n",
       "      <td>-1.519409e-12</td>\n",
       "      <td>5.957500e-21</td>\n",
       "      <td>-4.483071e-13</td>\n",
       "      <td>7.671563e-22</td>\n",
       "      <td>3.415176e-13</td>\n",
       "      <td>1.013220e-23</td>\n",
       "      <td>4.919873e-13</td>\n",
       "      <td>4.847925e-23</td>\n",
       "      <td>think2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    F3 alpha mean  F3 alpha psd_mean  F3 beta mean  F3 beta psd_mean  \\\n",
       "0   -3.513743e-13       3.726133e-22 -2.682559e-14      3.288808e-23   \n",
       "1    2.641169e-12       4.587082e-21 -9.250070e-13      5.641363e-22   \n",
       "2   -1.110026e-12       9.769720e-21  9.964152e-14      4.792419e-22   \n",
       "3   -1.985656e-12       2.873449e-21  2.032677e-13      2.973924e-22   \n",
       "4    6.528760e-12       3.527670e-20 -1.353519e-12      1.106202e-21   \n",
       "5   -1.379280e-12       4.423076e-20  1.223617e-12      1.180685e-21   \n",
       "6   -4.587835e-12       4.330082e-20  1.727180e-12      3.195574e-21   \n",
       "7    1.329518e-12       3.092606e-20 -5.441050e-13      3.797635e-21   \n",
       "8   -1.464838e-12       3.998122e-20  7.614474e-14      9.044639e-21   \n",
       "9   -6.682706e-13       4.353180e-20  1.798419e-12      6.372825e-21   \n",
       "10  -1.675986e-11       4.661575e-20 -4.681929e-12      7.481881e-21   \n",
       "11   4.920095e-12       4.483697e-20  4.948356e-13      6.069827e-21   \n",
       "\n",
       "    F4 alpha mean  F4 alpha psd_mean  F4 beta mean  F4 beta psd_mean  \\\n",
       "0   -9.515559e-14       5.562550e-21 -5.775861e-13      7.106336e-22   \n",
       "1    7.079107e-12       3.216913e-20 -5.932923e-14      3.037872e-21   \n",
       "2   -4.087955e-12       8.125469e-20  5.467051e-13      4.184450e-21   \n",
       "3   -6.230472e-12       3.910542e-20  1.097228e-12      3.867061e-21   \n",
       "4    9.481486e-12       4.706429e-20 -1.541731e-12      1.674903e-21   \n",
       "5   -3.906943e-12       8.441385e-20  1.021773e-12      2.495726e-21   \n",
       "6   -3.870291e-12       4.189717e-20  1.403395e-12      3.225566e-21   \n",
       "7    1.053348e-12       2.256651e-20 -1.160170e-13      2.483586e-21   \n",
       "8   -2.269418e-12       3.074925e-20 -4.057946e-13      6.314228e-21   \n",
       "9   -2.334771e-12       2.913618e-20 -3.185245e-12      3.965142e-21   \n",
       "10   3.538875e-12       2.861831e-20  1.164432e-12      4.348650e-21   \n",
       "11  -2.728418e-12       2.033302e-20 -4.279024e-13      2.608493e-21   \n",
       "\n",
       "    C3 alpha mean  C3 alpha psd_mean  ...  Fp1 beta psd_mean  Fp2 alpha mean  \\\n",
       "0   -6.227103e-13       6.441535e-21  ...       1.301683e-21    2.104581e-12   \n",
       "1    1.046693e-12       7.630595e-22  ...       1.351102e-21    1.886877e-11   \n",
       "2   -1.788294e-12       1.903302e-20  ...       1.484600e-21   -9.369546e-12   \n",
       "3   -2.167441e-12       8.275744e-21  ...       1.339161e-21   -1.426302e-11   \n",
       "4    4.119992e-13       1.018550e-22  ...       1.603727e-21    1.986007e-11   \n",
       "5    2.212569e-13       7.981192e-22  ...       1.377544e-21   -7.881025e-12   \n",
       "6   -5.293694e-13       1.091890e-21  ...       1.432964e-21   -1.996213e-11   \n",
       "7    3.385722e-13       7.190826e-23  ...       1.303909e-21    1.977747e-12   \n",
       "8   -2.429279e-12       3.773124e-21  ...       1.277685e-21    1.529181e-12   \n",
       "9   -8.880180e-13       1.680661e-21  ...       1.423573e-21    5.059446e-13   \n",
       "10   6.727053e-12       1.658058e-21  ...       1.378970e-21   -1.121567e-12   \n",
       "11  -8.722756e-13       7.994848e-22  ...       1.489864e-21   -1.519409e-12   \n",
       "\n",
       "    Fp2 alpha psd_mean  Fp2 beta mean  Fp2 beta psd_mean  O1 gamma mean  \\\n",
       "0         3.491705e-19  -8.972747e-12       3.815544e-20   1.462627e-12   \n",
       "1         4.980549e-19  -1.780069e-12       4.427011e-20  -4.275238e-13   \n",
       "2         2.936030e-19   1.199184e-12       1.653221e-20   1.045572e-14   \n",
       "3         1.362751e-19   1.125618e-12       1.455048e-20   9.151191e-16   \n",
       "4         2.720955e-19  -3.152343e-12       8.206781e-21  -7.601483e-14   \n",
       "5         3.264408e-19   2.327577e-12       8.140498e-21  -3.391651e-14   \n",
       "6         7.977403e-19   1.147115e-11       4.953088e-20   1.190654e-13   \n",
       "7         4.945291e-19  -1.645501e-12       4.143383e-20  -2.415048e-13   \n",
       "8         1.355524e-20   9.406878e-13       2.428333e-21  -6.834998e-14   \n",
       "9         1.122695e-20  -1.420028e-12       1.473201e-21  -1.842385e-14   \n",
       "10        1.047478e-20  -1.432469e-13       1.557606e-21   1.671160e-13   \n",
       "11        5.957500e-21  -4.483071e-13       7.671563e-22   3.415176e-13   \n",
       "\n",
       "    O1 gamma psd_mean  O2 gamma mean  O2 gamma psd_mean        task  \n",
       "0        8.390667e-23  -1.834673e-13       1.259452e-24  med1breath  \n",
       "1        1.377218e-22  -2.358534e-13       1.524009e-23        med2  \n",
       "2        6.581794e-27  -9.197855e-14       7.589472e-24      think1  \n",
       "3        4.195413e-27  -1.023871e-13       3.604797e-24      think2  \n",
       "4        9.007884e-25   1.798727e-14       1.490968e-24  med1breath  \n",
       "5        1.063922e-24  -8.547079e-14       2.196102e-23        med2  \n",
       "6        3.554555e-23   2.474980e-13       2.225727e-23      think1  \n",
       "7        5.554593e-24  -4.687246e-14       1.324148e-24      think2  \n",
       "8        3.473931e-24   1.198150e-13       6.617865e-23  med1breath  \n",
       "9        1.510362e-24   4.958239e-13       2.337877e-23        med2  \n",
       "10       9.341824e-24  -5.916038e-14       2.708215e-23      think1  \n",
       "11       1.013220e-23   4.919873e-13       4.847925e-23      think2  \n",
       "\n",
       "[12 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b9e59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = main_df.drop('task', axis = 1)\n",
    "y = main_df['task']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e204ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_min = min_max_scaler.fit_transform(X)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_min, y_encoded, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "070d69a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create a Random Forest classifier (you can try other classifiers as well)\n",
    "clf = LogisticRegression(random_state=11)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1254cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "preds = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bf694bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siddarth\\anaconda3\\envs\\py3tf2\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "C:\\Users\\Siddarth\\anaconda3\\envs\\py3tf2\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:30:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Standardize the features (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d573e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create an SVM classifier\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26ef328a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Print the importance of each feature\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature, importance \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m, feature_importances):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimportance\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the test data (optional)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Print the importance of each feature\n",
    "for feature, importance in zip(X.columns, feature_importances):\n",
    "    print(f'{feature}: {importance:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define a neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(len(class_labels), activation='softmax')  # Output layer with softmax activation\n",
    "])\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # Use categorical cross-entropy\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Convert target labels to one-hot encoding\n",
    "y_train_one_hot = keras.utils.to_categorical(y_train, len(class_labels))\n",
    "y_test_one_hot = keras.utils.to_categorical(y_test, len(class_labels))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_one_hot, epochs=100, batch_size=64, validation_split=0.2, callbacks=[callback])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert predicted labels back to string labels\n",
    "predicted_labels = [class_labels[np.argmax(pred)] for pred in predictions]\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed776070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Assuming you have your data loaded as X_train, y_train, X_test, y_test\n",
    "num_unique_values = X_train.shape[0]\n",
    "input_sequence_length = 28\n",
    "embedding_dim = 1\n",
    "# Define the CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=num_unique_values, output_dim=embedding_dim, input_length=input_sequence_length),\n",
    "    layers.Conv1D(128, 5, activation='relu'),\n",
    "    layers.MaxPooling1D(2),\n",
    "    layers.Conv1D(64, 5, activation='relu'),\n",
    "    layers.MaxPooling1D(2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f755613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test contains true labels and predicted_labels contains predicted labels\n",
    "# These should be NumPy arrays or Python lists.\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels_encoded)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaea34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Replace these with your actual predicted and true labels\n",
    "predicted_labels = y.unique()\n",
    "true_labels = y.unique()\n",
    "\n",
    "# Get the unique class names from the labels\n",
    "class_names = np.unique(true_labels)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predicted_labels, labels=class_names)\n",
    "\n",
    "# Create a heatmap to visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# You can also print a classification report with precision, recall, and F1-score\n",
    "print(classification_report(true_labels, predicted_labels, target_names=class_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3tf2)",
   "language": "python",
   "name": "py3tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
